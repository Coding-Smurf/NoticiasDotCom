import streamlit as st
import json
from datetime import datetime
from utils.news_scraper import NewsScraper

st.set_page_config(
    page_title="Dashboard - Monitor de Noticias",
    page_icon="ğŸ“Š",
    layout="wide",
)

# Initialize scraper
if 'scraper' not in st.session_state:
    api_key = st.secrets.get("OPENAI_API_KEY", "")
    st.session_state.scraper = NewsScraper(api_key)

scraper = st.session_state.scraper

st.title("ğŸ“Š Dashboard de Noticias")

# Sidebar configuration
with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")
    
    recent_days = st.slider(
        "DÃ­as recientes",
        min_value=1,
        max_value=90,
        value=30,
        help="Filtrar noticias de los Ãºltimos N dÃ­as"
    )
    
    max_workers_sites = st.slider(
        "Workers (Sitios)",
        min_value=1,
        max_value=10,
        value=5,
        help="Sitios procesados en paralelo"
    )
    
    max_workers_articles = st.slider(
        "Workers (ArtÃ­culos)",
        min_value=1,
        max_value=20,
        value=10,
        help="ArtÃ­culos procesados en paralelo"
    )
    
    max_workers_summaries = st.slider(
        "Workers (ResÃºmenes)",
        min_value=1,
        max_value=10,
        value=5,
        help="ResÃºmenes generados en paralelo"
    )
    
    request_delay = st.number_input(
        "Delay entre requests (seg)",
        min_value=0.0,
        max_value=5.0,
        value=1.0,
        step=0.5
    )

# Update config
scraper.update_config({
    "recent_days": recent_days,
    "max_workers_sites": max_workers_sites,
    "max_workers_articles": max_workers_articles,
    "max_workers_summaries": max_workers_summaries,
    "request_delay": request_delay,
})

# Main content
st.markdown("---")

# File upload
uploaded_file = st.file_uploader(
    "ğŸ“ Sube tu archivo CSV con las fuentes",
    type=['csv'],
    help="El CSV debe tener columnas: siteURL, web (1 para activado)"
)

if uploaded_file:
    st.success(f"âœ… Archivo cargado: {uploaded_file.name}")
    
    if st.button("ğŸ” Buscar Noticias", type="primary", use_container_width=True):
        with st.spinner("ğŸ”„ Procesando sitios..."):
            # Progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Load sites from uploaded file
            sites = scraper.load_sites_from_file(uploaded_file)
            status_text.text(f"ğŸ“ {len(sites)} sitios cargados")
            progress_bar.progress(10)
            
            if not sites:
                st.error("âŒ No se encontraron sitios vÃ¡lidos en el CSV")
            else:
                # Process sites
                status_text.text("ğŸŒ Extrayendo noticias...")
                all_news = scraper.process_all_sites(sites)
                progress_bar.progress(50)
                
                if not all_news:
                    st.warning("âš ï¸ No se encontraron noticias recientes")
                else:
                    # Group and summarize
                    status_text.text("ğŸ”— Agrupando duplicados...")
                    groups = scraper.group_duplicates(all_news)
                    progress_bar.progress(70)
                    
                    status_text.text("ğŸ“ Generando resÃºmenes...")
                    all_news = scraper.summarize_groups(groups, all_news)
                    progress_bar.progress(100)
                    
                    # Store results
                    st.session_state.results = all_news
                    st.session_state.groups = groups
                    
                    status_text.text("âœ… Proceso completado")
                    st.balloons()

# Display results
if 'results' in st.session_state and st.session_state.results:
    st.markdown("---")
    st.header("ğŸ“Š Resultados")
    
    results = st.session_state.results
    groups = st.session_state.groups
    
    # Stats
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“° Total ArtÃ­culos", len(results))
    with col2:
        unique_sources = len(set(article.get('source', '') for article in results))
        st.metric("ğŸŒ Fuentes", unique_sources)
    with col3:
        st.metric("ğŸ“‘ Grupos", len(groups))
    with col4:
        duplicates = sum(1 for g in groups if len(g) > 1)
        st.metric("ğŸ”— Duplicados", duplicates)
    
    # Display groups
    st.markdown("### ğŸ“‹ Grupos de Noticias")
    
    for group_idx, group_indices in enumerate(groups, 1):
        with st.expander(
            f"**Grupo {group_idx}** - {len(group_indices)} artÃ­culo(s)",
            expanded=(len(group_indices) > 1)
        ):
            # Summary
            first_article = results[group_indices[0]]
            if first_article.get('summary'):
                st.info(f"**Resumen:** {first_article['summary']}")
            
            # Articles
            for i in group_indices:
                article = results[i]
                
                st.markdown(f"""
                **{article['title']}**
                - ğŸ”— [{article['url']}]({article['url']})
                - ğŸ“ Fuente: `{article['source']}`
                - ğŸ“… Fecha: {article.get('date', 'N/A')}
                """)
                
                if i < group_indices[-1]:
                    st.markdown("---")
    
    # Download button
    st.markdown("---")
    
    json_str = json.dumps(results, ensure_ascii=False, indent=2)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    st.download_button(
        label="ğŸ’¾ Descargar JSON",
        data=json_str,
        file_name=f"noticias_{timestamp}.json",
        mime="application/json",
        use_container_width=True
    )

else:
    st.info("ğŸ‘† Sube un archivo CSV y haz clic en 'Buscar Noticias' para comenzar")